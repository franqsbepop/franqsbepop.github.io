<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <meta name="description" content="Diogo Franquinho - Blog Post 13">
    <title>Mathematical Philosophy and Large Language Models</title>
    <link rel="stylesheet" href="../css/style.css">
</head>
<body>
    <header>
        <nav class="top-nav">
            <h2>
                <a href="../index.html#biography">Biography</a> | 
                <a href="../index.html#research">Research</a> | 
                <a href="../index.html#publications">Publications</a> | 
                <a href="../index.html#teaching">Teaching</a> | 
                <a href="../blog.html">Blog</a> | 
                <a href="../index.html#contact">Contact</a>
            </h2>
        </nav>
        <div class="container"></div>
    </header>

    <main>
        <article>
            <h2>Mathematical Philosophy and Large Language Models: New Frontiers in Mathematical Reasoning</h2>
            <p>Date: December 2, 2024</p>

            <p>
                The rapid advance of Large Language Models (LLMs) has brought fresh insight to both the philosophy of mathematics and the role of artificial intelligence in mathematical practice. State-of-the-art systems such as OpenAI's GPT-4 and Google's Gemini can now engage with complex mathematical language, generate solution outlines, and even reason through portions of proofs, casting new light on the relationship between formal logic and natural language<sup>[1]</sup>.
            </p>

            <p>
                In contrast to traditional computer algebra systems, LLMs are designed to converse naturally about mathematics: they explain concepts, propose strategies, and adapt to a wide range of topics—all through language. Research (for example, <em>Measuring Mathematical Problem Solving With the MATH Dataset</em><sup>[3]</sup> and <em>Training Verifiers to Solve Math Word Problems</em><sup>[4]</sup>) reveals both their potential and their limits: although LLMs can be surprisingly helpful in problem-solving dialogues, they often struggle with intricate, abstract, or multi-step mathematical reasoning.
            </p>

            <p>
                This merging of human-like conversational ability with mathematical content challenges traditional notions of mathematical understanding and communication. LLM-generated proofs and informal arguments promise more approachable explanations, yet also introduce new sources of error and ambiguity. These developments force us to revisit foundational epistemological questions: What does it mean to "understand" mathematics? How should we judge machine-generated explanations or proofs? Such debates echo the concerns of philosophers like Imre Lakatos and Penelope Maddy, highlighting the delicate boundary between recognizing patterns and achieving true comprehension<sup>[5]</sup>.
            </p>

            <p>
                As LLMs continue to evolve, their influence on mathematical methodology and philosophical inquiry is likely to deepen. Issues at the very foundation—such as the nature of mathematical objects, the validity of proofs, or the interplay between syntax and semantics—may gain new context as the boundary blurs between formal manipulation and the flexible, dialogic nature of mathematical conversation. Understanding this interplay will be central not only to AI research but also to the future philosophy of mathematics<sup>[2]</sup>.
            </p>

            <section id="references">
                <h3>References</h3>
                <ol>
                    <li id="ref1"><a href="https://arxiv.org/pdf/2303.08774">OpenAI. (2023). GPT-4 Technical Report. arXiv:2301.10848.</a></li>
                    <li id="ref2"><a href="https://plato.stanford.edu/entries/philosophy-mathematics/">Shapiro, S. (2020). Philosophy of Mathematics. The Stanford Encyclopedia of Philosophy.</a></li>
                    <li id="ref3"><a href="https://arxiv.org/pdf/2103.03874">Hendrycks, D., et al. (2021). Measuring Mathematical Problem Solving With the MATH Dataset. arXiv:1904.01557.</a></li>
                    <li id="ref4"><a href="https://arxiv.org/pdf/2110.14168">Cobbe, K., et al. (2022). Training Verifiers to Solve Math Word Problems. arXiv:2206.14858.</a></li>
                </ol>
            </section>
        </article>
    </main>

    <footer>
        <p>&copy; Diogo Franquinho | Blog</p>
    </footer>
</body>
</html>
